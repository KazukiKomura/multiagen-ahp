# AAMAS向け：理由ベース合意形成システム 実装・研究計画 v2（アップデート案）

> 本書は、提示いただいた実装計画をベースに AAMASレベルの学術貢献と再現性・厳密性を強化するためのアップデート版です。理論（形式化）・アルゴリズム・実験計画・エンジニアリング・倫理/再現性の 5 軸で改善しています。
> 

了解！できるだけ短く、初学者向けにまとめました。

- **理想**：少ない理由で、短時間に、公平に、納得して決まる。
- **現状**：理由が数字に反映されず、合意が遅く不安定。
- **課題**：納得された理由だけで評価を更新し、決定を安定させる。
- **問題**：価値観の衝突と矛盾で、結論が揺れ、説明が弱い。
- **アプローチ**：理由の採用を判定 → 採用分だけ重み・点数を微調整 → 提示順は学習で最適化。

---

## 0. エグゼクティブサマリ

- **核となる主張**: 「**価値（基準重み）を明示するAHP** × **価値に依存する構造化論証（VAF系）** × **学習的戦略選択** を **同一の固定点ダイナミクス**として統合。受理された論証集合が誘導する“支持ベクトル”により AHP を**正当化付きで更新**し、合意と説明可能性を同時に最適化する。
- **新規性**:
    1. **Argument-Grounded AHP Update (AGAU)** という**理論的更新子**を定義（重み/スコア/整合性に対する 3 種の更新子）。
    2. **価値依存型論証意味論（VAF）と AHP の往復依存**を**反復固定点問題**として解き、**収束性条件**と**単調性**の命題を提示。
    3. **戦略学習（RL / バンディット）で論証タイプ・対象選択を最適化、説得効率と合意質**の同時改善を実証。
- **主要成果物**: 形式化・疑似コード・収束条件の命題、強力なベースライン比較、頑健性/アブレーション、再現性パッケージ（Docker＋Artifact）を含む。

### 0.1 既存研究の限界と本研究の新規性（要旨）

- *多基準合意（AHP/CRP）**の限界：理由（論証）との連動が弱く、更新の正当化が不透明。CI是正はあるが“誰のどの理由”で構造が変わったか追跡困難。
- *形式論証（Dung/VAF）**の限界：受理集合までは出せるが、数値的選好（重み・スコア）への写像が未統合。AHPとの往復依存が扱われない。
- **学習的説得（RL/バンディット）の限界：勝てる戦略の同定はできるが、合意の数値構造**や**説明責任**へ落ちない。
- **本研究の新規性（再掲）**：AGAU（論証起点のAHP更新子）＋価値依存意味論×AHPの**固定点ダイナミクス**＋学習的戦略選択（LinUCB/TS/Q）により、**合意効率・質・説明可能性**を同時最適化。強ベースラインとアブレーションで検証。

### 0.2 実世界対応づけ（マッピング）

| 研究要素 | 実務での意味 | 価値 |
| --- | --- | --- |
| AHP構造（重み/スコア/CI） | 調達・政策・PJ評価の基準表/比較表 | 重視軸の可視化と一貫性管理 |
| 論証グラフ（VAF） | 議事録/コメントの理由関係（支持/攻撃） | 紛争点の構造化・透明性 |
| AGAU更新 | **受理理由**のみが数値構造を動かす | 「理由が意思決定を動かす」可追跡化 |
| 学習的戦略選択 | 論点の出す順番の最適化 | 会議回数・疲弊の削減 |
| 説明指標（最小説明集合等） | 監査/説明責任資料 | 合意の正当化・再利用性 |

### 0.3 研究フロー（FMT：Sim事前学習→凍結→被験者）

1. **シミュレーション事前学習**：マルチエージェント環境で LinUCB/TS（補助でQ学習）を学習。
2. **OPEで安全性検査**：IPS/DR 等で**悪化しない方策**のみ採択（CI/Kendall-τ/Gini の悪化が閾値超→不採用）。
3. **ポリシー凍結（Full-frozen）**：探索率0・seed/commit固定。被験者実験の Full 条件は凍結方策を用いる。
4. **パイロット→本実験**：パイロットでUI/所用時間調整の後、被験者内3条件（Baseline/Explain-only/Full-frozen）をラテン方格で実施。拡張として Full-adaptive を別アームで評価可。

---

## 1. 形式化（Theory）

**用語整理**：「エージェント」は二層。①**進行役AI**（学習し、報酬で方策更新）と、②**参加者エージェント(1人or複数)**（受理/保留/棄却でAHPが更新されるが学習はしない）。VAFは価値順序に基づき採否を裁定し、採用理由のみAGAUでw/S/Pを微更新する。

### 1.1 モデルの構成要素

- **評価空間**: 代替案集合 𝒜、基準集合 𝒞、重み w∈Δ∣C∣−1\mathbf{w} \in \Delta^{|\mathcal{C}|-1}。スコア行列 S∈R≥0∣A∣×∣C∣\mathbf{S} \in \mathbb{R}_{\ge 0}^{|\mathcal{A}|\times |\mathcal{C}|}。
- **論証グラフ**: G=(V,Eatt,Esup)G=(V,E_{att}, E_{sup})。各論証 v∈Vv \in V は `(topic, target, proposed, confidence, impact)` を持つ。価値（基準重み）に依存した**攻撃/支援**関係をもつ（Value-based AF）。
- **受理集合**: 価値順序（重みから誘導）に基づく意味論（grounded 既定）で**受理された論証集合** Acc(w)Acc(\mathbf{w})。

### 1.2 AGAU：論証に基づく AHP 更新子

- **(i) 基準重み更新（Criteria_Disagreement）**
    - 受理論証から得る**符号付き支持ベクトル** s∈[−1,1]∣C∣\mathbf{s} \in [-1,1]^{|\mathcal{C}|}。
    - **指数的単純形保存更新**:
        
        w′  =  Norm(w⊙exp⁡(η s))\mathbf{w}' \;=\; \mathrm{Norm}\big( \mathbf{w} \odot \exp(\eta\,\mathbf{s}) \big)
        
        - η>0\eta>0 はステップ幅、`Norm` は L1 正規化、`⊙` は要素積。
- **(ii) 代替案スコア更新（Alternative_Reevaluation）**
    - 対象セル (a,c)(a,c) に対する符号付き支持 ma,cm_{a,c}。
    - **局所指数更新**: S′=S⊙exp⁡(η M)\mathbf{S}' = \mathbf{S} \odot \exp(\eta\, \mathbf{M})（M\mathbf{M} は零行列で対象にのみ非零）。
    - 新しい分析的信念: Bana′=S′ w′\mathbf{B}_{ana}' = \mathbf{S}'\,\mathbf{w}'。
- **(iii) 整合性是正（Self_Inconsistency）**
    - 基準 c のペア比較行列 P(c)\mathbf{P}^{(c)} に対して、固有ベクトル u\mathbf{u} から**最近接一貫行列** P~\tilde{\mathbf{P}} を log 空間でブレンド：
        
        log⁡Pij′=(1−λ) log⁡Pij+λ log⁡(u~i/u~j),    λ∈(0,1]\log P'_{ij} = (1-\lambda)\,\log P_{ij} + \lambda\, \log(\tilde{u}_i/\tilde{u}_j),\;\; \lambda\in(0,1]
        
    - **CI 低減**かつ reciprocity 保持。

### 1.3 固定点ダイナミクス

1. 現在の w\mathbf{w} から価値順序を誘導し **Acc(w\mathbf{w})** を算出。
2. Acc に基づき s,M,λ\mathbf{s}, \mathbf{M}, \lambda を生成、**AGAU** で (w,S,P)(\mathbf{w},\mathbf{S},\mathbf{P}) を更新。
3. 収束判定（∥w′−w∥1<ϵ\lVert \mathbf{w}'-\mathbf{w}\rVert_1 < \epsilon か、Acc が一定）。

**命題 A（単調性）**: 受理集合が単調に「基準 k の支持のみ」与える期間、wkw_k は単調増加。

**命題 B（収束条件の一例）**: 受理集合が有限ステップで安定し、η<1/L\eta<1/L（L は Lipschitz 上界）ならば、AGAU は L1 ノルムで縮小写像となり収束する。

> 証明方針: 指数更新の凸性と L1 正規化、受理集合の停止性（grounded 意味論の無循環性）を用いる。
> 

---

## 2. アルゴリズム設計（Pseudo-code）

### 2.1 環境ステップ（簡略）

```python
# Inputs: state = (w, S, P, argument_graph), agents
# Output: next_state, logs

for agent in agents:
    strat = agent.select_strategy(state)  # ε-greedy / Thompson
    arg = agent.generate_argument(state, strat)
    argument_graph.add(arg)

accepted = grounded_vaf(argument_graph, values=w)

s = build_support_vector(accepted)      # for weights
M = build_support_matrix(accepted)      # for scores
lam = inconsistency_gain(accepted, P)   # for pairwise

w_next = normalize(w * np.exp(eta * s))
S_next = S * np.exp(eta * M)
P_next = blend_pairwise(P, lam)         # log-space blending

return (w_next, S_next, P_next, argument_graph), logs

```

### 2.2 エージェント学習

- **選択肢**: Q学習（離散戦略×対象）、LinUCB / Thompson Sampling（コンテキスト：CI、分散、過去の受理率など）。
- **報酬**: (i) 受理確率 / 受理後の合意スコア改善、(ii) ダイアログあたりの**満足度改善/ターン**、(iii) 論証コスト控除。

---

## 3. 実験設計（AAMAS品質）

### 3.1 研究課題（RQs）

- **RQ1**: AGAU は（非論証ベースの）重み合意法より**合意の質**と**説明可能性**を同時に改善するか？
- **RQ2**: 価値依存意味論（VAF）を用いた受理制御は、受理集合非依存（Dung標準）より**説得効率**（turns to consensus）を向上させるか？
- **RQ3**: 学習的戦略は固定戦略より**ロバスト**（ノイズ/敵対/動的環境）か？

### 3.2 指標

- **合意効率**: 収束ターン数 / 失敗率 / 再開回数。
- **合意質**: 平均満足度、**Pareto 改善率**、社会厚生（Σ 満足度）、**Gini 不平等**。
- **正当化**: 受理論証により説明できる**改善割合**、論証ネットワークの**最小説明集合**サイズ。
- **AHP品質**: CI の推移、再現誤差、Kendall-τ による順位安定性。
- **公平性/バイアス**: 基準/代替案ごとの不均衡、個人間の regret 分布。

### 3.3 ベースライン

1. **DeGroot** 重み合意（連続平均化）。
2. **Kemeny-Young 近似**によるペア比較集約（社会選択）。
3. **CRP（Consensus Reaching Process）**：フィードバック型合意形成（AHP更新なし）。
4. **Dung AF** の受理のみで更新なし（説明のみ）。
5. **ランダム/頻度ベース** 論証選択。

### 3.4 アブレーション

- AGAU の (i)(ii)(iii) を個別無効化。
- 学習 → 貪欲/固定。
- VAF → 非価値依存意味論。

### 3.5 頑健性

- **ノイズ**: 観測ノイズ、通信エラー、ラベル汚染。
- **敵対**: 攻撃的論証スパム、戦略的虚偽、カルテル風同調。
- **動的**: 途中参加/離脱、価値転換、代替案の追加/廃止。

### 3.6 被験者実験（方法）

**目的**：Full（VAF+AGAU+凍結ポリシー）が Baseline（AHPのみ）/Explain-only（VAFのみ更新なし）に対して、合意**効率**・**質**・**説明可能性**・**公平性**を改善するか検証。

**参加者**：社会人/大学院生  n=24目標,最小n=12でパイロット開始n=24 目標, 最小 n=12 でパイロット開始。報酬 30–45 分で 1,000–2,000 円相当。除外：教示理解<80%、注意チェック不通過、操作エラー。

**倫理**：同意取得・匿名化・撤回自由。IRB 同等の審査を受ける。

**デザイン**：被験者内 3 条件（順序はラテン方格）。

- **Baseline（AHP-only）**：ペア比較/CI 是正のみ。
- **Explain-only（VAF）**：受理集合は提示するが AHP 更新なし。
- **Full-frozen**：受理集合に基づく **AGAU** で重み・スコア・CI を更新。提示順は**事前学習→OPE 合格→凍結**した LinUCB/TS ポリシーで決定。

> 拡張：Full-adaptive（保守的探索；ε≪1）を別アームで追加可。
> 

**タスク/シナリオ**：1 テーマ×3 ラウンド（各条件）。

- 題材例：医療機器調達/自治体 EV 充電網/企業 PJ 選定（代替案=5、基準=5）。
- 目標：最終順位の確定と**最小説明集合**の確認。各ラウンド 12–15 分、最大 200 ステップ。

**UI/フロー**：導入（ミニチュートリアル）→初期化（直感的重み/スコア入力）→条件別対話→終了（説明ビュー）。Full では参加者が**受理/保留/棄却**を選択し、受理集合に基づき AGAU を反映。

**測定指標**

- **客観**：収束ターン数、失敗率、再開回数／平均満足度・社会厚生・**Gini**／Kendall-τ（順位安定）／CI 推移／**最小説明集合**サイズと説明カバレッジ。
- **主観**（7段階）：理解度・納得感・公平性認知・透明性・信頼／NASA-TLX／使用意図。

**サンプルサイズ/検出力**：主効果は Full vs Baseline（主要 DV=収束ターン、説明集合）。効果量想定 dz=0.5、n=24 で 1−β≈0.8。逐次**ベイズ判定**（BF10>3）で段階的増加、frequentist は OBF 型 α-spending。

**分析計画**：

- 線形混合効果モデル（条件=固定、参加者/題材=ランダム）、ブートストラップ CI と Wilcoxon を併記、Holm 補正。
- 効果量：Cohen’s dz / r、Cliff’s δ。操作チェック通過者のみの再解析を付録に。

**成功基準（仮説）**：

- H1: Full は Baseline より**収束が速い**。
- H2: Full は Explain-only より**最小説明集合が小さい**。
- H3: Full は主観的**公平性/納得感/信頼**が高い。
- H4: Full は **CI 非増加**かつ **Kendall-τ 高**。

## **事前学習と OPE**：被験者前に MA シミュレーションで LinUCB/TS を学習し、OPE（IPS/DR）で**悪化なし**を確認の上でポリシー凍結。seed/commit を記録し再現可能化。

## 4. エンジニアリング強化

### 4.1 実装構成（差分）

```
reason_based_consensus/
├── core/
│   ├── semantics.py           # VAF/Dung/grounded 実装
│   ├── agau.py                # AGAU 更新子（w, S, P）
│   └── proofs/                # 命題の補助実装（検証スクリプト）
├── agents/
│   ├── bandits.py             # LinUCB/TS
│   └── rewards.py             # 報酬関数群
├── environment/
│   ├── generators.py          # 初期状態/敵対者/ノイズ生成
│   └── dashboards.py          # 進行可視化（streamlit/plotly）
├── experiments/
│   ├── hydra_conf/            # Hydra 設定
│   ├── artifact_pack/         # 再現性バンドル作成
│   └── stats.py               # 効果量/有意性/多重補正
└── utils/
    ├── validation.py          # pydantic schema + Hypothesis property-tests
    └── determinism.py         # 乱数固定・ハッシュ化

```

### 4.2 ベストプラクティス（追加）

- **設定管理**: Hydra/OmegaConf、`configs/` に条件別 yaml。Run すべてを MLflow/W&B に自動記録。
- **検証**: Hypothesis による**性質テスト**（例：指数更新の単純形保持、 reciprocity 保持）。
- **最適化**: Numba/JIT または JAX の選択ビルド、ペア比較固有値に**安定化ガード**（正則化/クリッピング）。
- **リソース**: 並列 Ray + バッチ化、結果は Parquet で永続化。
- **可観測性**: 構造ログ（JSONL）、論証グラフのスナップショット、CI/重み/満足度の時系列。

---

## 5. 再現性・アーティファクト

- **Artifact 包**: `docker compose up` で主要実験が 1 コマンド再現。固定シード 5 本、結果 CSV/図表を自動生成。
- **匿名化**: AAMAS 向け double-blind スクリプト（Git 履歴除去・著者名/組織名検査）。
- **チェックリスト**: データ可用性、計算資源、測定の不確かさ、ハイパーパラメータ、停止規則、失敗ケースの開示。

---

## 6. 評価に向けた理論パート（ドラフト）

- **定義**（抜粋）
    - *正当化整合性*: 受理集合が誘導する支持でのみ更新された w\mathbf{w} により、現時点の最良代替案の選好変化が**受理集合の議論**で説明できること。
    - *合意安定性*: 時間 tt と t+Δt+\Delta で Kendall-τ が阈値以上、かつ CI が非増加。
- **命題 C（正当化の十分条件）**: 受理集合の支持が単一基準 k に集中し、(i) その基準に対するスコア差が閾値を超え、(ii) CI が非増加であるならば、最良代替案の変化は受理集合により説明可能。
- **命題 D（敵対耐性の下界）**: 攻撃論証密度が ρ\rho 以下、ステップ幅 η\eta が O(1/ρ)O(1/\rho) のとき、合意失敗確率は exp⁡(−c/η)\exp(-c/\eta) で上から抑えられる（経験的下界）。

> 実験では C/D を経験則として支持する図表・信頼区間を提示（厳密証明は付録にスケッチ）。
> 

---

## 7. タイムライン（12 週間／開始時期可変）

- **W1–3**：シミュレーション事前学習（LinUCB/TS＋補助Q）／OPE（IPS/DR）で安全性確認／ポリシー選抜。
- **W4**：**ポリシー凍結**（探索率0、seed/commit固定）／被験者 UI 最終化。
- **W5**：**パイロット**（n=6–12）：UI摩擦、所要時間、効果量の推定。必要なら軽微な UI 調整→**再凍結**。
- **W6**：**事前登録**（OSF 等）：主要/副次評価指標、停止規則、除外基準、解析式。
- **W7–8**：**本実験**（被験者内 3 条件：Baseline / Explain-only / Full-frozen）。
- **W9–10**：解析（混合効果／ブートストラップ／有意性）・図表生成・失敗分析。
- **W11–12**：拡張実験（任意：**Full-adaptive**）／論文化（付録に OPE・凍結情報・再現手順）。

---

## 8. 採択率を上げるための執筆指針

1. **貢献の明確化**: AGAU（理論）/VAF連携（意味論）/学習（最適化）の 3 本柱を図解し、既存 GDM/AF との違いを表に整理。
2. **疑似コード + 複雑度**: 時間計算量とメモリ上界を明示、スケール実験とリンク。
3. **強いベースライン**: DeGroot/Kemeny/CRP をしっかり最適化して比較（弱い設定にしない）。
4. **失敗分析**: 収束しない/不公平/敵対に弱いケースを率直に提示、改善余地を議論。
5. **再現性**: ワンコマンド実行、図表再現の自動 Notebook、ランダム性の置信区間。

---

## 9. 具体的「次の一手」

- [ ]  `core/semantics.py` に grounded VAF の最小実装（NetworkX + 受理反復）。
- [ ]  `core/agau.py` に 3 種更新子を分離実装し、Hypothesis 性質テストを付与。
- [ ]  `agents/bandits.py` に LinUCB（特徴：CI, variance, accepted_rate）。
- [ ]  `experiments/hydra_conf/` で Baseline/Full/Adversarial の 3 プリセット。
- [ ]  **図表テンプレ**：学習曲線、受理論証ネットワーク、CI/満足度の二軸時系列、フェアネス箱ひげ。

---

### 付録A: データスキーマ差分（抜粋）

```python
Argument = {
  "id": str,
  "sender": int,
  "topic": "Self_Inconsistency" | "Criteria_Disagreement" | "Alternative_Reevaluation",
  "target": {"component": "weight"|"score"|"pairwise", "index": tuple},
  "proposed": float | dict,
  "confidence": float,         # [0,1]
  "impact": {"sign": int, "magnitude": float},
  "relations": {"attacks": list[str], "supports": list[str]}
}

```

### 付録B: 主要ハイパーパラメータ

- ステップ幅 η\eta（重み/スコア別に別管理）。
- CI 是正強度 λ\lambda（Self_Inconsistency で使用）。
- 学習：ε、α、UCB 探索係数、報酬スケーリング。

### 付録C: 疑似コード（受理計算の骨子）

```python
def grounded_vaf(G, values):
    # values -> value ordering over criteria
    undec, acc, rej = set(G.V), set(), set()
    changed = True
    while changed:
        changed = False
        for v in list(undec):
            if all(u in rej for u in attackers(v, values)):
                acc.add(v); undec.remove(v); changed = True
        for v in list(undec):
            if any(u in acc for u in attackers(v, values)):
                rej.add(v); undec.remove(v); changed = True
    return acc

```

---

**本アップデート案はそのまま実装仕様として利用可能**です。必要に応じて、ターゲット会議（AAMAS/JAAMAS/他）に合わせたフォーマット・ページ制限・Artifact チェックリストを追補します。

# 関連研究（草案）

本研究は、(i) 抽象／価値ベース論証（AF/VAF）、(ii) 合意形成プロセス（CRP／LSGDM）、(iii) 交渉・説得の強化学習、(iv) 論証的XAI／説明推薦 の4系譜を束ね、AHPに基づく多基準重み付けと構造化論証の生成・更新を統合した、**理由ベース合意形成**を提案する。

**抽象論証と価値ベース拡張**：Dung(1995)に端を発する抽象論証は受容可能性の計算基盤を与え、Bench‑Capon(2003)のVAFは価値（価値観）に依存する受容性を導入した。決定のための論証（Amgoud & Prade, 2009）は、論証を選好・説明に結び付ける一般枠組を与え、本研究の「理由→更新→合意」の背骨となる。

**合意形成（CRP, LSGDM）**：CRP研究は社会ネットワーク上の信頼や意見進化にもとづく重み付け・再調整を軸に発展してきた（Dong 2018; García‑Zamora 2022 など）。しかし多くは“数値的な距離”に基づく助言／懐柔ヒューリスティクスで、**具体的に何が誰の信念構造をどう変えたか**はブラックボックスに近い。本研究はAHPの一貫性指数（CI）やHCI等で**構造的不整合を可視化**し、そこに**構造化論証**（Self‑Inconsistency／Criteria‑Disagreement／Alternative‑Reevaluation）を差し込む点が新しい。これにより「誰のどの基準・代替案に効いたか」を追跡できる。

**交渉・説得の強化学習**：交渉対話へのRL適用（Georgila & Traum 2011; Hiraoka et al. 2014; Lewis et al. 2017 など）は、ユーザシミュレータ上で方策を学習し、説得行為（フレーミング等）の有効性を示してきた。とはいえ、既存研究は**行為継続の報酬設計**や会話戦略学習に重心があり、**多基準の構造的信念更新**や**CI/満足度といった意思決定構造の健全性指標**まで同時に最適化する枠組は乏しい。本研究はAHP×論証×RLの三位一体化で、**どの論証トピックをいつ誰に投げるか**を環境フィードバック（合意率、CI低下、満足度上昇）で学習させる。

**論証的XAI／説明推薦**：近年、推薦・説明では、グラフ上の論証スキャフォールドから**相互作用的説明**を生成する手法が台頭（Rago 2018, 2020, 2021; Vassiliades 2021）。本研究はこれらを「**説得・合意形成へ拡張**」し、説明を**能動的に信念を動かすための理由操作**として再解釈する。説明の個別化と更新履歴の紐付けにより、**説明→行動変容→構造更新**の閉ループを実現する。

**本研究の新規性**：

- AHPの**一貫性検査（CI/HCI等）を、論証生成・適用のトリガ**として利用（不整合の“原因部位”に直接介入）。
- Topic別の**構造化論証→選択的更新**で、どの基準・代替案・重みをどれだけ動かしたかを**理由つきで可視化**。
- RLで**論証トピック選択・発話タイミング**を学習し、**合意率・満足度・CI低下**を並立指標として最適化。
- CRP系の“距離最小化”に対し、**理由に根差した構造更新**で**説明責任**と**頑健性**を両立。

---

# 参考文献（BibTeX）

```
@article{Dung1995AI,
  author  = {Phan Minh Dung},
  title   = {On the Acceptability of Arguments and its Fundamental Role in Nonmonotonic Reasoning, Logic Programming and n-Person Games},
  journal = {Artificial Intelligence},
  year    = {1995},
  volume  = {77},
  number  = {2},
  pages   = {321--357},
  doi     = {10.1016/0004-3702(94)00041-X}
}

@article{BenchCapon2003VAF,
  author  = {Trevor J. M. Bench-Capon},
  title   = {Persuasion in Practical Argument Using Value-based Argumentation Frameworks},
  journal = {Journal of Logic and Computation},
  year    = {2003},
  volume  = {13},
  number  = {3},
  pages   = {429--448},
  doi     = {10.1093/logcom/13.3.429}
}

@article{AmgoudPrade2009AI,
  author  = {Leila Amgoud and Henri Prade},
  title   = {Using Arguments for Making and Explaining Decisions},
  journal = {Artificial Intelligence},
  year    = {2009},
  volume  = {173},
  number  = {3-4},
  pages   = {413--436},
  doi     = {10.1016/j.artint.2008.11.006}
}

@article{Vassiliades2021KER,
  author  = {Alexis Vassiliades and Nick Bassiliades and Thanos Patkos},
  title   = {Argumentation and Explainable Artificial Intelligence: A Survey},
  journal = {The Knowledge Engineering Review},
  year    = {2021},
  volume  = {36},
  pages   = {e2},
  doi     = {10.1017/S0269888921000011}
}

@inproceedings{Cyras2021IJCAI,
  author    = {Kyrillas Cyras and Francesca Toni and others},
  title     = {Argumentative XAI: A Survey},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21), Survey Track},
  year      = {2021}
}

@article{Rago2021AIJ,
  author  = {Antonio Rago and Oana Cocarascu and Christos Bechlivanidis and David Lagnado and Francesca Toni},
  title   = {Argumentative Explanations for Interactive Recommendations},
  journal = {Artificial Intelligence},
  year    = {2021},
  volume  = {296},
  pages   = {103506},
  doi     = {10.1016/j.artint.2021.103506}
}

@inproceedings{Rago2020KR,
  author    = {Antonio Rago and Oana Cocarascu and Christos Bechlivanidis and Francesca Toni},
  title     = {Argumentation as a Framework for Interactive Explanations for Recommendations},
  booktitle = {Proceedings of KR 2020},
  year      = {2020},
  pages     = {805--815},
  doi       = {10.24963/kr.2020/83}
}

@inproceedings{Rago2018IJCAI,
  author    = {Antonio Rago and Oana Cocarascu and Francesca Toni},
  title     = {Argumentation-based Recommendations: Fantastic Explanations and How to Find Them},
  booktitle = {Proceedings of IJCAI 2018},
  year      = {2018},
  pages     = {1949--1955}
}

@inproceedings{Lewis2017EMNLP,
  author    = {Mike Lewis and Denis Yarats and Yann Dauphin and Devi Parikh and Dhruv Batra},
  title     = {Deal or No Deal? End-to-End Learning of Negotiation Dialogues},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2017},
  pages     = {2443--2453},
  doi       = {10.18653/v1/D17-1259}
}

@inproceedings{GeorgilaTraum2011Interspeech,
  author    = {Kallirroi Georgila and David Traum},
  title     = {Reinforcement Learning of Argumentation Dialogue Policies in Negotiation},
  booktitle = {Proceedings of INTERSPEECH 2011},
  year      = {2011},
  address   = {Florence, Italy}
}

@article{MonteserinAmandi2013ESWA,
  author  = {Ariel Monteserin and Anal{\'\i}a Amandi},
  title   = {A Reinforcement Learning Approach to Improve the Argument Selection Effectiveness in Argumentation-based Negotiation},
  journal = {Expert Systems with Applications},
  year    = {2013},
  volume  = {40},
  number  = {14},
  pages   = {3561--3571},
  doi     = {10.1016/j.eswa.2012.10.045}
}

@article{Dong2018KBS,
  author  = {Yucheng Dong and Qinghua Zha and Hong Zhang and Gang Kou and Hamido Fujita and Francisco Chiclana and Enrique Herrera-Viedma},
  title   = {Consensus Reaching in Social Network Group Decision Making: Research Paradigms and Challenges},
  journal = {Knowledge-Based Systems},
  year    = {2018},
  volume  = {162},
  pages   = {3--13},
  doi     = {10.1016/j.knosys.2018.06.036}
}

@article{GarciaZamora2022JAS,
  author  = {Diego Garc{\'\i}a-Zamora and {\'A}lvaro Labella and Weiping Ding and Rosa M. Rodr{\'\i}guez and Luis Mart{\'\i}nez},
  title   = {Large-Scale Group Decision Making: A Systematic Review and a Critical Analysis},
  journal = {IEEE/CAA Journal of Automatica Sinica},
  year    = {2022},
  volume  = {9},
  number  = {6},
  pages   = {949--966},
  doi     = {10.1109/JAS.2022.105617}
}

@article{SteinMizzi2007EJOR,
  author  = {William E. Stein and Philip J. Mizzi},
  title   = {The Harmonic Consistency Index for the Analytic Hierarchy Process},
  journal = {European Journal of Operational Research},
  year    = {2007},
  volume  = {177},
  number  = {1},
  pages   = {488--497},
  doi     = {10.1016/j.ejor.2005.10.057}
}

@article{Pant2022Math,
  author  = {Sangeeta Pant and Anuj Kumar and Mangey Ram and Yury Klochkov and Hitesh K. Sharma},
  title   = {Consistency Indices in Analytic Hierarchy Process: A Review},
  journal = {Mathematics},
  year    = {2022},
  volume  = {10},
  number  = {8},
  pages   = {1206},
  doi     = {10.3390/math10081206}
}

@inproceedings{Hiraoka2014COLING,
  author    = {Takuya Hiraoka and Graham Neubig and Sakriani Sakti and Tomoki Toda and Satoshi Nakamura},
  title     = {Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing},
  booktitle = {Proceedings of COLING 2014},
  year      = {2014},
  pages     = {1706--1717}
}

@inproceedings{Donadello2022AAAI,
  author    = {Ivan Donadello and others},
  title     = {Machine Learning for Utility Prediction in Argument-Based Computational Persuasion},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2022}
}

```

> 追加希望の文献（合意形成の最新モデルやVAFの応用、交渉RLの転移・一般化など）があればお知らせください。体裁（BibTeXキーや著者表記）は投稿スタイルに合わせて調整します。
>