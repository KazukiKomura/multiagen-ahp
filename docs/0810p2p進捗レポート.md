# 0810 p2p.py 学習・検証 進捗レポート

## 実行概要

**日時**: 2025-08-10  
**実行コマンド**: 
```bash
PYTHONHASHSEED=0 python p2p.py \
  --steps_per_update 128 --updates 150 \
  --pi_epochs 6 --eval_every 10 --val_steps 64 \
  --save_dir artifacts/v1 --check_invariants 1 --export_templates
```

**実行環境**:
- Python 3.9 + PyTorch
- 固定seed (123) + PYTHONHASHSEED=0 による完全再現性
- macOS Darwin 24.4.0

## 学習結果

### ベストモデル性能 (upd=80で保存)
- **Social Interest (J)**: 0.0869 (正の収束 ✅)
- **単一基準支配度 (D_final)**: 0.251 (閾値 < 0.6 を大幅クリア ✅)
- **重み分散中央値**: 0.023 (極めて良好な合意形成)

### 学習ログ抜粋
```
[eval @upd 80] J=0.0869  D_final=0.251
>> saved BEST: J=0.0869 to {'pt': 'artifacts/v1/policy_v1_seed123_best.pt', 'ts': 'artifacts/v1/policy_v1_seed123_best.ts'}
```

後半 (upd=150) では J=-0.1042, D_final=0.630 と悪化したため、**early-stopping で upd=80 版を採用**。

## 成果物検証

### 1. 保存モデル
```bash
$ ls -lh artifacts/v1/policy_v1_seed123_best.*
-rw-r--r-- policy_v1_seed123_best.pt   171K  # PyTorch state_dict
-rw-r--r-- policy_v1_seed123_best.ts   177K  # TorchScript (本番用)
```

### 2. モデル動作確認
```python
import torch
policy = torch.jit.load('artifacts/v1/policy_v1_seed123_best.ts')
# Input: [batch_size, 24] 観測ベクトル
# Output: [batch_size, 180] アクション logits
# ✅ 正常動作、形状一致確認済み
```

### 3. エクスポートファイル
- `action_space.json`: 180アクション定義 (w:30, S:150, 各符号90個ずつ)
- `templates.json`: UI用日本語テンプレート

### 4. 受理率パターン (学習時)
- **S (選択肢調整)**: +1=97.6%, -1=34.7% (正常: 上げ提案が通りやすい)
- **w (重み調整)**: +1=87.3%, -1=25.6% (正常範囲内)

### 5. 重み分散推移
- 中央値 0.023 (目標 < 0.6 を大幅下回る)
- 最大値 0.127 (単一基準への極端集中なし)
- 時系列安定: 前半0.027 → 後半0.026

## バージョン管理

**Production v1.0 として確定**:
- Git tag: `v1.0` 
- Commit: `f74b10e`
- README.md に性能指標・ファイルパス記載完了

## p2p.py 詳細仕様

### アーキテクチャ概要
- **多エージェントAHP**: 各エージェントが基準重み `w` と選択肢評価 `S` を保持
- **VAF (Value-based Argumentation Framework)**: 個人レベルでの論証受理判定
- **AGAU (AHP-aware Gradual Update)**: 受理された論証による重み・評価の段階的更新
- **PPO学習**: 「どの論証を提案するか」の方策を強化学習

### 主要コンポーネント

#### 1. NodeState (エージェント状態)
```python
@dataclass
class NodeState:
    w: np.ndarray            # (C,) 基準重み
    S: np.ndarray            # (A,C) 代替案×基準の評価行列
    Pcrit: np.ndarray        # (C,C) 基準間ペア比較行列
    Palt: List[np.ndarray]   # 基準ごとの代替案ペア比較行列
    CIcrit: float            # 基準重みの整合性指標
    veto_crit: set           # Vetoクライテリア
    w_floor: np.ndarray      # 重み下限 (0.05)
    tau: np.ndarray          # 評価下限 (0.05)
```

#### 2. 安全機構
- **Veto**: 重要基準の重み削減を阻止
- **Floor**: 重み・評価の下限値維持 (`project_simplex_with_floor`)
- **CI監視**: 整合性悪化時は更新幅を縮小

#### 3. 観測ベクトル (24次元)
```python
obs = [w_i(5), w_nei_mean(5), |w_i - w_nei_mean|(5), w_nei_std(5), 
       last_si, last_cons, CIcrit, H(w_i)]
# C=5の場合: 5+5+5+5+4 = 24次元
```

#### 4. アクション空間 (180次元)
- **w調整**: 5基準 × 2符号 × 3強度 = 30アクション
- **S調整**: 5基準 × 5代替案 × 2符号 × 3強度 = 150アクション

#### 5. 報酬設計
```python
reward = 1.0 * d_si + 0.3 * d_cons + 0.2 * own_accepted - 0.5 * D_penalty
```
- `d_si`: Social Interest変化 (個人効用と集団効用の相関向上)
- `d_cons`: 合意度向上 (近傍との重み乖離減少)  
- `own_accepted`: 自論受理
- `D_penalty`: 単一基準支配抑制

### 学習フロー
1. **PPOロールアウト** (`steps_per_update=128`)
2. **環境ステップ** (`step_env`):
   - 全エージェント同時行動
   - VAF個人受理判定 
   - AGAU更新 (重み・評価)
   - Gossip (重み平滑化)
   - 報酬計算
3. **PPO更新** (`pi_epochs=6`)
4. **評価** (`eval_every=10`): 学習OFF短期ロールアウト

### 新機能 (今回追加)
- **安全機構統計**: veto/floor/tau阻止回数のログ出力
- **invariant check**: 状態整合性の実行時検証
- **early-stopping**: 評価悪化時のベストモデル保存
- **テンプレートエクスポート**: Web実験用UI素材出力

## 次回アクション

### Web実験準備
1. **サーバ実装**: 24次元観測ベクトル組み立て → TorchScript推論
2. **UI実装**: `templates.json` → カード化インターフェース
3. **パイロット**: n=30-40 → 本番 n=100-150/条件

### 技術的詳細
- **モデルロード**: `torch.jit.load('artifacts/v1/policy_v1_seed123_best.ts')`
- **推論モード**: サンプリング (多様性) vs argmax (決定論的)
- **ログ必須項目**: session_id, group_id, action_idx, accepted, safety_events

### 分析準備
- CSV → 可視化パイプライン
- 受理率ヒートマップ
- 重み収束・SI推移グラフ
- A/B条件間比較

---

**結論**: upd=80で保存されたモデル (J=0.0869, D_final=0.251) を **Production v1.0** として確定。Web被験者実験に進行可能。