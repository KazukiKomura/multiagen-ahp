# フル版（事前学習）仕様書 v1 — 理由ベース合意形成システム

> 目的：**進行役AI（Mediator）**を事前学習し、**凍結ポリシー**として被験者実験に投入できる状態にする。対象は**フル版**（VAF×AHP×AGAU×文脈付きRL×マルチエージェント）。

---



## 1. スコープ / 成功基準

### 1.1 スコープ

* 事前学習は**シミュレーションのみ**（人データ不使用）。
* 本仕様は**Mediatorモード**を主対象（Audience-dependent は付録）。
* 生成物は**凍結方策**＋**OPE安全証明**＋**再現アーティファクト**。

### 1.2 成功基準（Pretraining Done）

* OPE（IPS/DR）で **悪化なし**（CI↑/Gini↑/収束遅延が閾値超え無し）。
* シミュ評価で Baseline 比：**収束ターン -30% 以上**、**最小説明集合 -40% 以上**。
* 乱数5 seed×3シナリオで結果が**安定**（標準誤差が平均の10%以下）。

---

## 2. 役割・用語

* **進行役AI（ポリシー）**：誰に・どの論点を・どの順で・どの強度で出すか選ぶ学習主体。
* **参加者エージェント（シミュ）**：個々の嗜好/矛盾/応答特性を持つ擬似人間。
* **VAF（審判）**：価値順序（重み）に基づいて論証の採否を裁定。
* **AHP（台帳）**：重み $\mathbf{w}$、スコア $\mathbf{S}$、ペア比較 $\mathbf{P}$ と CI を保持。
* **AGAU（更新子）**：受理理由だけで $w/S/CI$ を微更新する規則。

---

## 3. 環境モデル（Mediator）

### 3.1 状態（Environment State）

* 集団統計：$\bar w$（重み平均）、重み分散、平均CI、順位安定度（平均 Kendall-τ）。
* 個人特徴：各参加者 $i$ の $w_i, CI_i$、上位案、上位2案の対立度、受理履歴のEMA。
* 履歴：過去K手の提示ログ（topic/target/recipient/accept）。

### 3.2 行動（Action）

* **topic** ∈ {Self（矛盾是正）, Criteria（重み）, Alt（セル再評価）}
* **recipient** ∈ {agent\_j｜unicast} ∪ {ALL｜broadcast}
* **target**：topicに応じて

  * Self：基準 c のペア比較（i,j）
  * Criteria：基準 k
  * Alt：セル (a,k)
* **intensity**：微小な連続値（更新幅の提案）∈ \[0, 1]（実際の反映は安全柵内）

### 3.3 遷移（1ターン）

1. ポリシーが行動 a\_t を選択。
2. その行動から**論証カード**を生成・提示。
3. **VAF**が価値順序（$\bar w$）で採否を決定 → 受理集合 Acc。
4. **AGAU**で受理分のみ $w/S/P$ を更新（宛先には強く、他メンバーに弱く適用）。
5. 指標を更新（Kendall-τ, CI, Gini, 説明集合サイズ等）。

### 3.4 停止規則（Episode End）

* トップ案不変×2 & 平均 Kendall-τ ≥ 0.85
* 連続5手で受理0
* ステップ上限 T\_max（デフォルト200）

---

## 4. 参加者シミュレータ

### 4.1 嗜好・不整合の初期化

* 真の嗜好ベクトル：$w_i^* \sim Dirichlet(\alpha)$
* 初期申告重み：$w_i$ は $w_i^*$ にノイズ（温度 τ）をかけて正規化。
* CI 初期化：各基準のペア比較にランダム揺らぎ（目標 CI\* に向けて分布）。

### 4.2 受理確率モデル（概念）

* **価値整合**：提案が推す価値と $w_i$ の整合度。
* **証拠強度**：カードの confidence / magnitude。
* **疲労/反発**：同型カードの反復時に減衰。
* 受理確率：$\sigma(\theta_1 \cdot \text{整合} + \theta_2 \cdot \text{証拠} - \theta_3 \cdot \text{反復})$。

### 4.3 更新の拡散

* 宛先 i に対して $\eta_{high}$（例 0.05）、その他 j≠i に $\eta_{low}$（例 0.01）。

---

## 5. VAF・AGAU の要約

### 5.1 VAF（grounded, 価値順序依存）

* 価値順序は $\bar w$ から誘導。
* 攻撃は、攻撃側の価値が被攻撃側以上に優先されるときのみ有効。

### 5.2 AGAU（指数更新 + logブレンド）

* 重み：`w' = Norm(w * exp(η_w * s))`
* スコア：`S' = S * exp(η_S * M)`
* ペア比較：`log P' = (1-λ) log P + λ log P~`（CI低減）。

---

## 6. ポリシー学習

### 6.1 文脈（特徴量）

* 集団：max/mean CI、重み分散、上位案の割れ度、平均受理率EMA、残ステップ比。
* 個人：対象候補に対する乖離度 $\|w_i-\bar w\|$、CI\_i、最近の受理履歴。

### 6.2 アルゴリズム

* **主**：LinUCB（線形報酬仮定；不確実性で探索） or Thompson（線形ガウス/ベータ）。
* **補**：Q学習（段取り最適化、割引 γ∈\[0.9,0.99]）。

### 6.3 報酬（step-wise）

$r_t = \alpha\,\Delta\text{progress} + \beta\,\Delta\text{satisfaction} - \gamma\,\Delta\text{Gini} - \delta\,\Delta\text{CI} - \lambda$

* progress：Kendall-τ 上昇 or トップ案安定化。
* satisfaction：社会厚生（Σ効用）上昇。
* 係数例：$\alpha=1.0, \beta=0.5, \gamma=0.5, \delta=0.3, \lambda=0.01$。

### 6.4 安全柵

* $\eta_w, \eta_S$ の上限、$\lambda$ の上限（例 ≤0.1）。
* 重大悪化（CI↑やGini↑が閾値超）時は**ロールバック**。
* 受理0が続く場合は**方策抑制**（探索縮小）。

---

## 7. OPE（Off-Policy Evaluation）

### 7.1 ログと傾向度

* 学習時、**行動確率**（propensity）を必ず記録：$\hat \pi_b(a|x)$。

### 7.2 推定器

* IPS：$\hat V_{IPS}(\pi) = \frac{1}{N}\sum w_t r_t$,  $w_t = \frac{\pi(a_t|x_t)}{\hat \pi_b(a_t|x_t)}$。
* Doubly Robust（推奨）：モデルベース予測とIPSを併用、**分散とバイアスを低減**。
* クリッピング：$w_t \le W_{max}$（例 50）。

### 7.3 安全基準

* 主要DV（収束ターン、CI、Gini）で **悪化なし**（上限を満たす）を満たすポリシーのみ**凍結**。

---

## 8. 実装インタフェース

```
reason_based_consensus/
├── core/
│   ├── semantics.py      # VAF（grounded, 価値順序依存）
│   ├── agau.py           # w/S/P 更新・安全柵・ロールバック
│   └── measures.py       # Kendall-τ, CI, Gini, 説明集合
├── agents/
│   ├── policy.py         # LinUCB/TS/Q、行動サンプリングと学習
│   ├── argument_gen.py   # (topic, target, intensity)→論証カード生成
│   └── recipient_pick.py # 宛先ヒューリスティック/学習
├── environment/
│   ├── simulator.py      # 参加者モデル、VAF→AGAU→指標更新ループ
│   └── generators.py     # 初期w_i, CI_i, ノイズ/敵対者の生成
├── experiments/
│   ├── train.py          # 事前学習エントリ（Ray並列対応）
│   ├── evaluate.py       # シミュ評価 & OPE
│   └── hydra_conf/       # YAML 設定（下記例）
└── utils/
    ├── logging.py        # JSONL構造ログ（propensity含む）
    └── determinism.py    # 乱数固定・ハッシュ
```

---

## 9. Hydra 設定例（抜粋）

```yaml
# experiments/hydra_conf/config.yaml
experiment:
  name: pretrain_full_v1
  seeds: [0,1,2,3,4]
  episodes: 500
  parallel: 8

env:
  mode: mediator
  n_agents: 5
  n_alternatives: 5
  n_criteria: 5
  Tmax: 200
  eta_w: 0.05
  eta_S: 0.03
  lambda_max: 0.1
  diffuse: {to_recipient: 1.0, to_others: 0.2}

simulator:
  dirichlet_alpha: [1,1,1,1,1]
  weight_noise_tau: 0.2
  ci_target: 0.1
  accept_model: {theta1: 1.2, theta2: 0.8, theta3: 0.6}

policy:
  algo: linucb
  context: [max_ci, mean_ci, weight_var, top_split, ema_accept, step_left]
  ucb_alpha: 0.8
  gamma: 0.95
  actions:
    topics: [Self, Criteria, Alt]
    recipient: {mode: heuristic}
    target: {mode: learned, fallback: heuristic}

reward:
  w_progress: 1.0
  w_welfare: 0.5
  w_gini: 0.5
  w_ci: 0.3
  step_penalty: 0.01

ope:
  estimator: dr
  clip_w: 50
  accept_thresholds: {delta_ci_max: 0.0, delta_gini_max: 0.0, turn_ratio_min: 0.7}
```

---

## 10. 学習ループ（擬似コード）

```python
def train_one_episode(env, policy):
    x = env.reset()
    logs = []
    for t in range(env.Tmax):
        a, p = policy.select(x)   # action and propensity
        arg = env.argument_from(a)
        accepted = env.vaf_judge(arg)
        delta = env.agau_update(accepted)
        x_next, metrics = env.observe()
        r = reward(metrics)
        policy.update(x, a, r, x_next)  # LinUCB: online; Q: TD
        logs.append((x, a, r, p, metrics, delta))
        if env.should_stop(): break
        x = x_next
    return logs
```

---

## 11. ロギング / 可視化

* **構造ログ（JSONL）**：時刻 t ごとに `state_summary, action, propensity, accepted, agau_delta(w,S,CI), metrics`。
* **ダッシュボード**：学習曲線、Kendall-τ/CI/Gini の推移、受理ネットワーク、説明集合サイズ。

---

## 12. テスト計画

### 12.1 単体/性質テスト

* `w'` の単純形保持（∑=1, 非負）。
* `P'` の reciprocity（`P_ij * P_ji = 1`）。
* Self 受理回の **CI 非増加（期待値）**。
* ロールバック動作（悪化トリガの確認）。

### 12.2 統合テスト

* 少数（2–3人）で収束条件に達するか。
* 受理0の連続で停止するか。
* ログから OPE が再現計算できるか。

---

## 13. スケーリング / 複雑度

* VAF：スパース前提で準線形。
* AGAU：$O(n\,m)$。
* 並列：Ray でエピソード並列、シード×シナリオを水平分割。
* 計算目安：5人×500エピ×5seed ≈ 数時間（CPU16コア相当）。

---

## 14. 凍結と配布物

* `policy_frozen.pt`（重み/係数）＋ `config.yaml`（Hydra）＋ `commit_hash.txt`。
* `OPE_report.md`（悪化なしを証明、信頼区間付き）。
* `reproduce.sh`：Docker 1コマンド再実行。

---

## 15. 倫理・安全

* 事前学習は合成データのみ。
* 人手実験前に **OPEで悪化なし**のポリシーのみ採用、**探索0**。
* 更新幅の上限・ロールバック・監査ログ（どの理由がどれだけ動かしたか）を保持。

---

## 16. 付録：Audience-dependent 変形（要点）

* VAF採否とAGAU更新を**各受け手の w\_i** で個別実行。
* 収束判定：重みの平均 L1 距離が閾値以下、ペア Kendall-τ の平均が閾値以上。
* 計算は重くなるため、学習時は**サンプリング**（未選択メンバーは確率的更新）。

---

## 17. 用語簡易集

* **VAF**：価値順序で攻撃の有効性を決める“審判”ルール。
* **AHP**：基準重みと各案の点数で順位を決める“台帳”。
* **AGAU**：採用理由だけで台帳を**少しずつ**書き換える更新子。
* **LinUCB/TS**：文脈（状態）から、次に出す論点の**期待効果＋不確実性**で行動を選ぶバンディット。
* **OPE**：ログから**別方策の良さ**を安全に推定する手法（人相手に探索しない）。
